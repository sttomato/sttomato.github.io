<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[2017秋招——京东]]></title>
      <url>%2F2017%2F09%2F16%2F2017%E7%A7%8B%E6%8B%9B%E7%AC%AC%E4%B8%80%E5%9C%BA%EF%BC%9A%E4%BA%AC%E4%B8%9C%2F</url>
      <content type="text"><![CDATA[大量数据结构的问题 查找方式有哪些，都有哪些区别； 数组和链表的区别，分别是怎么实现的； 哈希表是如何实现的； 如何实现平衡树； 介绍一下红黑树；博客1，博客2 如何实现树的遍历； 汉诺塔问题 深度学习方面 过拟合、欠拟合； 过拟合有哪些解决方法； 欠拟合有哪些解决方法； GoogleNet； 怎么样去检测图像中的汽车； 深度学习有哪些超参数 哈希 构造合适的散列函数 除余法除数选用较大的素数，降低冲突发生的可能 MAD法 非递归版本先序遍历12345678910111213141516void PreOrder(TreeNode* root) &#123; stack&lt;TreeNode*&gt; s; TreeNode* p = root; while (p != NULL || !s.empty()) &#123; if (P != NULL) &#123; s.push(p); cout &lt;&lt; p -&gt; val &lt;&lt; endl; p = p -&gt; left; &#125; else &#123; p = s.top(); s.pop(); p = p -&gt; right; &#125; &#125;&#125; 中序遍历12345678910111213141516void InOrder(TreeNode* root) &#123; stack&lt;TreeNode*&gt; s; TreeNode* p = root; while (p != NULL || !s.empty()) &#123; if (P != NULL) &#123; s.push(p); p = p -&gt; left; &#125; else &#123; p = s.top(); s.pop(); cout &lt;&lt; p -&gt; val &lt;&lt; endl; p = p -&gt; right; &#125; &#125; &#125; 后序遍历 123456789101112131415161718192021void PostOrder(TreeNode* root) &#123;void InOrder(TreeNode* root) &#123; stack&lt;TreeNode*&gt; s; TreeNode* p = root; while (p != NULL || !s.empty()) &#123; if (P != NULL) &#123; s.push(p); p = p -&gt; left; &#125; else &#123; p = s.top(); s.pop(); if (! p -&gt; right) &#123; cout &lt;&lt; p -&gt; val &lt;&lt; endl; &#125; p = p -&gt; right; &#125; &#125; &#125; &#125;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[降维之PCA主成分分析原理]]></title>
      <url>%2F2017%2F09%2F15%2F%E9%99%8D%E7%BB%B4%E4%B9%8BPCA%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%E5%8E%9F%E7%90%86%2F</url>
      <content type="text"><![CDATA[背景在许多领域的研究与应用中，往往需要对反映事物的多个变量进行大量的观测，收集大量数据以便进行分析寻找规律。多变量大样本无疑会为研究和应用提供了丰富的信息，但也在一定程度上增加了数据采集的工作量，更重要的是在多数情况下，许多变量之间可能存在相关性，从而增加了问题分析的复杂性，同时对分析带来不便。如果分别对每个指标进行分析，分析往往是孤立的，而不是综合的。盲目减少指标会损失很多信息，容易产生错误的结论。 因此需要找到一个合理的方法，在减少需要分析的指标同时，尽量减少原指标包含信息的损失，以达到对所收集数据进行全面分析的目的。由于各变量间存在一定的相关关系，因此有可能用较少的综合指标分别综合存在于各变量中的各类信息。主成分分析与因子分析就属于这类降维的方法。 目的PCA（Principal Component Analysis）是一种常用的数据分析方法。PCA通过线性变换将原始数据变换为一组各维度线性无关的表示，可用于提取数据的主要特征分量，常用于高维数据的降维。能够对高维数据降维的算法包括： LASSO 主成分分析法 聚类分析 小波分析法 线性判别法 拉普拉斯特征映射 作用降维有什么作用呢？ 数据在低维下更容易处理、更容易使用； 相关特征，特别是重要特征更能在数据中明确的显示出来；如果只有两维或者三维的话，更便于可视化展示； 去除数据噪声 降低算法开销 常见的降维算法有主成分分析（principal component analysis,PCA）、因子分析（Factor Analysis）和独立成分分析（Independent Component Analysis，ICA）。 优化目标将一组N维向量降为K维（K大于0，小于N），其目标是选择K个单位（模为1）正交基，使得原始数据变换到这组基上后，各字段两两间协方差为0，而字段的方差则尽可能大（在正交的约束下，取最大的K个方差） 注意：PCA的变换矩阵是协方差矩阵，K-L变换的变换矩阵可以有很多种（二阶矩阵、协方差矩阵、总类内离散度矩阵等等）。当K-L变换矩阵为协方差矩阵时，等同于PCA。 原理最大化样本点在基上的投影，使得数据点尽量的分离。令第一主成分的方向是$u_1$，我们的目标就是将样本点在该方向上的投影最大化，即： \max {\frac{1}{n} \sum_{i=1}^n < u_1,x_i >^2} \sum_{i=1}^m (h_{\theta}(x^i)-y^i)^2$\sum_{i=0}^m$ $(h_{\theta}(x^{(i)})-y^{(i)})^2$ J(\theta)=\frac{1}{2m} \sum_{i=1}^{m} (h_{\theta}-y^{(i)}) J(\theta)=\frac{1}{2m}\sum_{i=1}^{m}\left(h_{\theta}(x^{(i)})-y^{(i)}\right)^2 \frac{1}{n}\sum_{i=1}^n < u_1, x_i > \rightarrow \frac{1}{n}\sum_{i=1}^n(x_1^Tu_1)^2=\frac{1}{n}\sum_{i=1}^n(x_1^Tu_1)^T(x_1^Tu_1) =\frac{1}{n} \sum_{i=1}^n(u_1^T x_1 x_1^T u_1)= \frac{1}{n}u_1^T \left( \sum_{i=1}^n x_1 x_1^T \right) u_1 = \frac{1}{n} u_1^T \left(XX^T \right) u_1其中的$X=[x_1,x_2,…,x_n]^T,x_i\in R^{m}$。那么优化函数就变成了： \max u_1^T\left(XX^T\right)u_1以上式子是个二次型，可以证明XX^T是半正定矩阵，所以上式必然有最大值。 \max u_1^T\left(XX^T\right)u_1=\max ||X^Tu_1||_2^2优化函数 max||Wx||_2 s.t. W^TW=I解释：==最大化方差同时最小化协方差==（PCA本质上是将方差最大的方向作为主要特征，并且在各个正交方向上将数据“离相关”）。最大化方差意味着，使得每个样本点在每个维度上与均值有很大差异，就是说非常有个性，有个性才能分辨出来；同时协方差越小的话表明样本之间的互相影响就非常小，如果协方差是0的话，表示两个字段完全独立。 寻找协方差矩阵的特征向量和特征值就等价于拟合一条能保留最大方差的直线或主成分。因为特征向量追踪到了主成分的方向，而最大方差和协方差的轴线表明了数据最容易改变的方向。根据上述推导，我们发现达到优化目标就等价于将协方差矩阵对角化：即除对角线外的其它元素化为0，并且在对角线上将特征值按大小从上到下排列。协方差矩阵作为实对称矩阵，其主要性质之一就是可以正交对角化，因此就一定可以分解为特征向量和特征值。 步骤总结一下PCA的算法步骤： 设有$m$条$n$维(字段数)数据。 将原始数据按列组成$n$行$m$列矩阵X. (行数代表字段数目，一个字段就是取每个样本的该维度的数值；列数代表样本数目) 将$X$的每一行（代表一个属性字段）进行零均值化，即减去这一行的均值 求出协方差矩阵$C=\frac{1}{m}XX^T$ 求出协方差矩阵的特征值及对应的特征向量 将特征向量按对应特征值大小从上到下按行排列成矩阵，取前k行组成矩阵P $Y=PX$即为降维到k维后的数据 去均值化的目的下面两幅图是数据做中心化（centering）前后的对比，可以看到其实就是一个平移的过程，平移后所有数据的中心是(0,0). 在做PCA的时候，我们需要找出矩阵的特征向量，也就是主成分（PC）。比如说找到的第一个特征向量是a = [1, 2]，a在坐标平面上就是从原点出发到点（1，2）的一个向量。如果没有对数据做中心化，那算出来的第一主成分的方向可能就不是一个可以“描述”（或者说“概括”）数据的方向了。还是看图比较清楚。 黑色线就是第一主成分的方向。只有中心化数据之后，计算得到的方向才能比较好的“概括”原来的数据。 限制PCA虽可以很好的解除线性相关，但是对于高阶相关性就没有办法了，对于存在高阶相关性的数据，可以考虑Kernel PCA，通过Kernel函数将非线性相关转为线性相关 参考 PCA的数学原理 K-L变换和PCA的区别 从PCA和SVD的关系拾遗 数据什么时候需要做中心化和标准化处理 主成分分析（PCA）原理详解]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Hello World]]></title>
      <url>%2F2017%2F09%2F14%2Fhello-world%2F</url>
      <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[快速排序]]></title>
      <url>%2F2017%2F09%2F13%2F%E5%BF%AB%E6%8E%92%E7%A8%8B%E5%BA%8F%2F</url>
      <content type="text"><![CDATA[博客上的代码1 博客上的代码2 1234567891011121314151617void quickSort(vector&lt;int&gt;&amp; array, int left, int right) &#123; int i = left; int j = right; int temp = array[left]; while (i != j) &#123; while (i &lt;j &amp;&amp; temp &lt;= array[j]) j --; if (i &lt; j) array[i] = array[j]; while (i &lt; j &amp;&amp; temp &gt; array[i]) i ++; if (i &lt; j) array[j] = array[i]; &#125; array[i] = temp; quickSort(array, left, i - 1); quickSort(array, i + 1, right);&#125; 分治思想 1234567891011121314151617181920212223int partition(vector&lt;int&gt;&amp; array, int left, int right) &#123; int i = left; int j = right; int temp = array[left]; while (i != j) &#123; while (i &lt;j &amp;&amp; temp &lt;= array[j]) j --; if (i &lt; j) array[i] = array[j]; while (i &lt; j &amp;&amp; temp &gt; array[i]) i ++; if (i &lt; j) array[j] = array[i]; &#125; array[i] = temp; return i;&#125;void quickSort(vector&lt;int&gt;&amp; array, int left, int right) &#123; if (left &lt; right) &#123; int dp = partition(array, left, right); quickSort(array, left, dp - 1); quickSort(array, dp + 1, right); &#125;&#125;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[机器学习修炼手册]]></title>
      <url>%2F2017%2F05%2F07%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%BF%AE%E7%82%BC%E6%89%8B%E5%86%8C%2F</url>
      <content type="text"><![CDATA[对机器学习的学习我开始于二年级的《数据挖掘》课，当时袁老师对数据挖掘中的常用的算法做了一些介绍，但是这仅仅是个入门教学，我并没有深入了解的其中的原理。到现在我才深刻的意识到ML的重要性，我就抽空看了一些这方面的资料，整理了这一份文档。 机器学习算法包括，监督学习(回归、分类)以及非监督学习(聚类)。 梯度下降\bbox[5px,border:2px solid red] { \theta_j:=\theta_j-\alpha\frac{\partial}{\partial \theta}J(\theta) }其中$\alpha$为学习率一般为很小的数字(0.001-0.1)，$\theta$为我们需要求解的参数，$J(\theta)$为能量函数或者为损失函数，通过上述公式可知，梯度下降是沿着损失函数梯度的反方向寻找迭代寻找最优值的过程。梯度下降容易陷入局部最极小点，所以我们要采取一定的措施来阻止这种现象的发生。 过拟合（Overfitting）如果训练样本的特征过多，我们学习的假设可能在训练集上表现地很好，但是在验证集上表现地就不尽人意 避免过拟合 减少特征的大小 正则化 在保证所有特征都保留的情况下，限制$\theta$的大小，即Small values for parameters $ \theta_0,\theta_1,\theta_2…\theta_n$ 当特征量很多时，该方式仍然表现的很好 交叉验证(Cross Validation) 正则化线性回归对于线性回归而言，其损失函数形式如下： J(\theta)=\frac{1}{2m}\sum_{i=1}^{m}\left(h_{\theta}(x^{(i)})-y^{(i)}\right)^2引入正则化之后的损失函数的形式为： J(\theta)=\frac{1}{2m}\left(\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})^2+\lambda\sum_{j=1}^{n}\theta_{j}^2\right)GD迭代求解参数Repeat{ \theta_0:=\theta_0-\alpha\frac{1}{m}\sum_{i=1}^{m}\left(h_{\theta}(x^{(i)})-y^{(i)}\right)x_0^{(i)} \theta_j:=\theta_j-\alpha\frac{1}{m}\left(\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})x_j^{(i)}+\lambda\theta_j\right)}梯度下降法的学习率$\alpha$需要提前指定，并且还要制定收敛标准。 Normal Equation \theta=\left(x^Tx+\lambda\begin{bmatrix} {0}&{0}&{\cdots}&{0}\\ {0}&{1}&{\cdots}&{0}\\ {\vdots}&{\vdots}&{\ddots}&{\vdots}\\ {0}&{0}&{\cdots}&{1}\\ \end{bmatrix}_{(n+1)(n+1)}\right)^{-1}x^Ty上式是对线性回归正则化后的矩阵解。可以证明的是当$\lambda&gt;0$时，求逆符号内部的式子总是可逆的。 逻辑回归在没有加入正则化之前，逻辑回归的损失函数的形式是这样的： J(\theta)=-\frac{1}{m}\sum_{i=1}^{m}\left(y^{(i)}\log\left(h_{\theta}(x^{(i)})\right)+(1-y^{(i)})\log\left(1-h_{\theta}(x^{(i)})\right)\right)加入正则项之后的形式为： J(\theta)=-\frac{1}{m}\sum_{i=1}^{m}\left(y^{(i)}\log\left(h_{\theta}(x^{(i)})\right)+(1-y^{(i)})\log\left(1-h_{\theta}(x^{(i)})\right)\right)+\frac{\lambda}{2m}\sum_{j=1}^{n}\theta_j^2GD迭代求解参数Repeat{ \theta_0:=\theta_0-\alpha\frac{1}{m}\sum_{i=1}^{m}\left(h_{\theta}(x^{(i)})-y^{(i)}\right)x_0^{(i)}\theta_j:=\theta_j-\alpha\frac{1}{m}\left(\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})x_j^{(i)}+\lambda\theta_j\right)} SVM支持向量机支持向量机又被称作最大间距（Large Margin）分类器，损失函数的形式是： J(\theta)=C\sum_{i=1}^{m}\left(y^{(i)}cost_1\left(h_{\theta}(x^{(i)})\right)+(1-y^{(i)})cost_0\left(h_{\theta}(x^{(i)})\right)\right)+\frac{1}{2}\sum_{j=1}^{n}\theta_j^2其中：$h_{\theta}(x^{(i)})=\theta^Tx^{i}$，$cost_1$以及$cost_0$的形式如下图所示： \begin{cases} \text{we want } \theta^Tx\ge1, & \text{if $y$ =1} \\[2ex] \text{we want } \theta^Tx\le-1, & \text{if $y$ =0} \end{cases}]]></content>
    </entry>

    
  
  
</search>
